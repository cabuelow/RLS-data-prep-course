---
title: "Cleaning ecological survey data for conservation scientists"
author: "CJ Brown"
date: "17 July 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#TODO: 
Recreate csv files with one duplicate siteid and one NA in the abundances
Create conshack url so it links!
Add an image at top


# Cleaning ecological survey data for conservation scientists

The most time-consuming step in many analyses is the cleaning and preparation of the data. This course will show you how you can use R to speed up that process and make it more transparent and repeatable. This means it will also be easier to check for and correct errors in your data processing. 

This course is part of a new initiative [Conservation Hackers](www.conservationhackers.org). Conservation hackers aims to connect conservation practitioners who need help with programming, data and data analysis with data scientists. Conservation hackers also aims to educate conservation scientists in programming skills. 

We're pleased to be partnering with [Reef Life Survey](https://reeflifesurvey.com/) to offer this course. Reef Life Survey is a not-for-profit organisation that trains volunteer citizen scientist divers in monitoring reef life. The Reef Life Survey data is freely available from their webpage and we'll be using it as an example of conservation monitoring data in this course. 

The course should take about 3 hours to complete. We'll focus on a particular set of packages for data analysis from the 'tidyverse'. We'll cover filtering, error checking, joinging data frames and some simple plots. The course aims to to explain some key concepts in detail so that students have the confidence to learn more. 

## Setup for this course  

So that the course runs efficiently, and to save plenty of time for trying fun things in R, we'd ask that you come to the course prepared. 

This is an intermediate level course, so we'll assume you know how to install R and R packages. As a general guide to what we expect in terms of prior knowledge, we'll assume you can run R, load data and write basic calculations that you save in a script. We'll assume you know nothing about how to structure data, create plots or make data summaries. Even if you know about these topics, you may still find the course helpful if you are self-taught, because we'll cover the conceptual foundation of these topics. 

Please have R and Rstudio installed on your computer before starting. You'll want to save plenty of time for doing this, it can be tricky on some computers, especially if you do not have 'admin' permission on a work computer. You may need to call IT to get help. We obviously only offer limited help with such installation issues. 
The latest version of R is version 4. You are welcome to try this version. We are using 3.6.2 currently for writing this course, so there may be some minor differences. The reason we haven't updated yet is that a lot of the packages we use haven't been updated yet. 

You'll also need to install a few R packages. We're using `dplyr`, `readr`, `lubridate`, `ggplot2` and `tidyr` in this course. You can install them with this R code: 
`install.packages(c("dplyr", "readr", "lubridate", "ggplot2"))` or just `install.packages(c("tidyverse"))`, which installs these packages and more. 

If that doesn't work email us with the error and we'll try to help. Otherwise, see your IT department for help. 
We're using dplyr version 1.0, which was released earlier this year. If you have an older version of dplyr the course should still work fine, but there may be some minor differences in the code. 

## Data and the case-study

We will use data from Reef Life Survey's Rottnest Island surveys (accessible [here]()). Rottnest Island is in Western Australia and its reefs host a diverse fish assemblage. This includes both temperate and tropical species. In 2011 a heatwave caused the fish community to undergo a massive change. We'll base our analysis today on a study by [Day and colleagues](https://onlinelibrary.wiley.com/doi/full/10.1111/ddi.12753), which showed an increase in tropical fish during and after the heatwave. 

## Overview of R in RStudio

We'll assume you are using R in Rstudio. Rstudio is an integrated developmement environment or IDE in hacker speak. The Rstudio IDE is an interface to R that adds many human friendly features. There are a few terms we'll assume you are familiar with in this course. First Rstudio's window panes: 

- Console: where the code is evaluated (computing) by the R program. 
- Script: Where you write your code so you can save it for later
- Environment: Basically the data in R's memory
- Object: An object in R's memory
- Class: Object's have different types, which means they interact with you and functions in different ways
- data frame: a class of R object commonly used for storing data, it has an equal number of rows,  column names and columns can vary in the class of data they store (e.g. dates, words, numbers). 

Some other important terms you should know: 

- R Package: a collection of code tools that we can import to add functionality to the base R language. Packages are stored in 'libraries' hence we read packages in with the `library()` command. 
- Function: a programming tool that has been preprepared for use in R. 
- Arguments: The parameters that a function uses to decide what to do. 
- .... other things


## The importance of being organized

So now you know Rstudio and have downloaded the data, let's talk about being organized. Organization is key, because even simple data analyses can produce a lot of complex code and many intermediate data outputs. 

We recommend you create a folder on your computer for each new project, like today's course. You can do this as an Rstudio project if you like (see File > New project) or just as a folder. Within that create a data-raw folder where you will keep your raw data. If datafiles are really big, then you might also have a folder somewhere else (e.g. on the drive) that you link to later. You can store intermediate data-steps in data-raw or another folder, its up to you. Also make a folder for your R code and a folder to save your figures in. 

We also recommend giving your files human and machine readable names. [Jenny Brian's](https://speakerdeck.com/jennybc/how-to-name-files) advice is a good to look at for ways to name your files. 

## New script

Now create a new script (file > New File > R Script) and save it with a sensible name in your project folder. 
Leave yourself some 'breadcrumbs' at the top of the script. It's common that you'll look at some R code after a long break and forget what you were doing. So start our scripts like this, with a title, name of creator, date and perhaps description: 


```{r}
# RLS data wrangling course
# CJ Brown 
#2020-07-23
```

The `#` symbol is a 'comment' that tells R to ignore this text. 

Now if your analysis gets complex you should split it among different scripts. We'll just use the one today, but if you have a more complex project its a good idea to chunk it. Then you might like to name you scripts '1_data-error-checking.R`, '2_exploratory-plots.R' etc..., so you know what order to run them in. 

## Packages  

Now we are organized, let's load a package into this session: 

```{r}
library(readr)
```

Much of R's power comes from all the user-contributed packages, which do a huge variety of different tasks. Many of these packages live in [CRAN](https://cran.r-project.org/) where they undergo some verification. There are many more on github and other repositories, these don't undergo the same level of checking. R's license requires that the user knows what they are doing (there is no warranty!), so its on you to read and understand what a package does. 

Today we'll use `readr` to read in data and then see some other packages for data wrangling and plotting. 

## Read in data

```{r}
dat <- read_csv("data-raw/Rottnest-Island-UVC.csv")
```

There is also the similar `read.csv()` in base R. We like to use readr's `read_csv` because it does some extra checks on the data, converts data to sensible classes where it can (e.g. dates), and allows for more flexibility in naming variables (columns). 

## Explore the data - CAB

For any new data you should always explore it. 

UP TO HERE IWTH WRITING

Importance of data type: numeric, factor, character, logical, etc...
```{r}
str(dat)
names(dat)
nrow(dat)
head(dat)
tail(dat)
length(unique(dat$SurveyID))
length(unique(dat$SpeciesID))
unique(dat$Sizeclass)
table(dat$Method)
```

## Data wrangling - CAB

```{r}
library(dplyr)
```

Overview of dplyr and utility for data wrangling
Pipe operator %>%?
select function might be a nice one to show too, alongside filter

## Filtering

Let's start by exploring data for one species, *Scarus ghobban*. 

```{r}
datscarus <- filter(dat, CURRENT_TAXONOMIC_NAME == "Scarus ghobban")
```

## First plots - CJB

```{r}
library(ggplot2)
```

```{r}
ggplot(datscarus) + 
  aes(x = Abundance) + 
  geom_histogram()
```

Explain theory of ggplot2
Grammar of graphics
Layering

```{r}
ggplot(datscarus) + 
  aes(x = Abundance, y = Biomass) +
  geom_point()
```


```{r}
ggplot(datscarus) + 
  aes(x = Abundance, y = Biomass, color = Sizeclass) +
  geom_point()
```

## If have time could go into color palettes here?

## Joining - CJB
```{r}
survdat <- read_csv("data-raw/Rottnest-Island-surveys.csv")
datscarus2 <- left_join(datscarus, survdat, by = "SurveyID")
```

Explain how to avoid bugs in joins
How data 'gains' happen - left joins and duplicating rows
How data 'losses' happen - incorrect names, etc...
Watch number of observations in global environment
Check number of unique observations for any loss/gain of data in joins

```{r}
nrow(datscarus)
nrow(datscarus2)
```

Dates are hard, one helpful package is lubridate.

```{r}
library(lubridate)
datscarus2$year <- year(datscarus2$SurveyDate)
```

```{r}
ggplot(datscarus2) + 
  aes(x = year, y = Abundance) + 
  geom_point()
```

```{r}
ggplot(datscarus2) + 
  aes(x = year, y = Abundance) + 
  geom_point() +
  stat_smooth()
```
Try turning off hte points,  you can see the trend better

## Join sites - CJB

- Add an error here. 


```{r}
sdat <- read_csv("data-raw/Rottnest-Island-sites.csv")
dat2 <- left_join(dat, survdat, by = "SurveyID")
nrow(dat)
length(unique(dat$SurveyID))
length(unique(dat2$SurveyID))
nrow(dat2)
dat2 <- left_join(dat2, sdat, by = "SiteCode")
nrow(dat2)
length(unique(dat2$SurveyID))
```


##Summarizing - CAB

```{r}
dat2$year <- year(dat2$SurveyDate)
dat2g <- group_by(dat2, SurveyID)
datA <- summarize(dat2g, 
                  sum_abund = sum(Abundance))

```

Now redo with `na.rm = TRUE`

```{r}
ggplot(datA) + 
  aes(x = sum_abund, color = NULL) +
  geom_histogram()
```

```{r}
dat3 <- filter(dat2, !is.na(TempTrop_23cutoff))
dat3g <- group_by(dat3, SurveyID, SiteCode, year, TempTrop_23cutoff)
dat4 <- summarize(dat3g, 
                  sum_abund = sum(Abundance, na.rm = TRUE))

```
```{r}
ggplot(dat4) + 
  aes(x = year, y = sum_abund, color = TempTrop_23cutoff) +
  geom_point() +
  stat_smooth()
```

## Creating new variables - CAB

```{r}
library(tidyr)
dat5 <- pivot_wider(dat4, names_from = TempTrop_23cutoff,
                    values_from = sum_abund,
                    values_fill = list(sum_abund = 0))
dat5 <- mutate(dat5, prop_trop = tropical/(temperate + tropical))

```
Opposite of `pivot_wider` is `pivot_longer`. 


```{r}
ggplot(dat5) + 
  aes(x = year, y = prop_trop) +
  geom_point(aes(color = SiteCode)) + 
  stat_smooth()
```


##

```{r}
dat6g <- group_by(dat5, SiteCode, year)
dat6 <- summarize(dat6g, prop_trop = mean(prop_trop))
```

```{r}
ggplot(dat6) + 
  aes(x = year, y = prop_trop, color = SiteCode) +
  geom_line()
```

## Can we throw some errors into the data to highlight common problems with joins, data types, etc... as going through examples above?

## Getting help - Both of us. 

Google is your friend! stack exchange etc.

## Conclusion

Table of other tidyverse tools you might wnat to check out

### Pipes 

### Lubridate 

### Dealing with strings 


